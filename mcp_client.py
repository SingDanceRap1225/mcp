import asyncio

from llm_model.ai_app import AIApp
from typing import Optional
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from utills import format_available_tools, handle_tool_call

"""åˆå§‹åŒ– MCP å®¢æˆ·ç«¯"""
class MCPClient:
    def __init__(self):
        self.app: AIApp = None
        self.session = None
        self.exit_stack = AsyncExitStack()
        self.session: Optional[ClientSession] = None

    """è¿æ¥åˆ°MCPæœåŠ¡å™¨å¹¶åˆ—å‡ºå¯ç”¨å·¥å…·"""
    async def connect_to_mcp_server(self, server_script_path: str):
        # å¯åŠ¨MCP Server
        command = "python"  # æš‚æ—¶ä»…æ”¯æŒpythonè„šæœ¬
        server_params = StdioServerParameters(command=command, args=[server_script_path], env=None)

        # ä¸MCP Serverå»ºç«‹é€šä¿¡
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        await self.session.initialize()

        # åˆ—å‡ºMCPServerä¸Šçš„å·¥å…·
        mcp_response = await self.session.list_tools()
        tools = mcp_response.tools
        print("å·²è¿æ¥MCPæœåŠ¡å™¨ï¼Œæ”¯æŒä»¥ä¸‹å·¥å…·ï¼š\n", [tool.name for tool in tools])

        available_tools = format_available_tools(mcp_response)
        # print(available_tools)

        # åˆå§‹åŒ–LLMï¼Œå¹¶å°†å¯ç”¨çš„MCPServerå·¥å…·æ³¨å…¥
        self.app = AIApp(config_file='llm_model/config.ini', tools=available_tools)

    """ä½¿ç”¨LLMæŸ¥è¯¢å¹¶è°ƒç”¨MCPServer"""
    async def process_query(self, query: str) -> str:
        response = self.app.generate_response(1, query)

        # å¤„ç†è¿”å›å†…å®¹
        content = response.choices[0]
        if content.finish_reason == "tool_calls":
            tool_name, tool_args = handle_tool_call(content)

            # æ‰§è¡ŒMCPServerå·¥å…·
            result = await self.session.call_tool(tool_name, tool_args)
            print(f"Calling tool {tool_name} with args {tool_args}")

            # å°†æ¨¡å‹è°ƒç”¨çš„serverå·¥å…·è¿”å›ç»“æœè¿åŒåˆå§‹æé—®å†å–‚ç»™å¤§æ¨¡å‹
            response = self.app.generate_response(2, query +"/n" +result.content[0].text)

        response_text = response.choices[0].message.content

        return  response_text

    """è¿è¡Œäº¤äº’å¼èŠå¤©å¾ªç¯"""
    async def chat_loop(self):
        print("\nMCP å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡º")

        while True:
            try:
                query = input("\nQuery: ").strip()
                if query.lower() == 'quit':
                    break
                response = await self.process_query(query)  # å‘é€ç”¨æˆ·è¾“å…¥åˆ° Qwen AI API
                print(f"\nğŸ¤– AI: {response}")
            except Exception as e:
                print(f"\nâš ï¸ å‘ç”Ÿé”™è¯¯: {str(e)}")

    """æ¸…ç†èµ„æº"""
    async def cleanup(self):
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
        # å¯åŠ¨å¹¶è¿æ¥MCP Server
        await client.connect_to_mcp_server("mcp_server/mcp_server_starter.py")
        await client.chat_loop()
    finally:
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())