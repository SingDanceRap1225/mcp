import asyncio
from llm_model.ai_app import AIApp
from typing import Optional, List, Dict
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from utills import format_available_tools, handle_tool_call


class MCPClient:
    def __init__(self, llm: AIApp, server_script_path: list[str]):
        # åˆå§‹åŒ–å¯¹è¯ LLM
        self.llm_app = llm

        # ç”¨äºç®¡ç†å¼‚æ­¥ä¸Šä¸‹æ–‡çš„é€€å‡ºæ ˆ
        self.exit_stack = AsyncExitStack()

        # MCP å®¢æˆ·ç«¯ä¼šè¯ï¼Œä»¥å­—å…¸å½¢å¼å­˜å‚¨ä¸åŒæœåŠ¡å™¨çš„ä¼šè¯ä¿¡æ¯
        self.mcp_session = {}

        # MCP æœåŠ¡å™¨è„šæœ¬è·¯å¾„
        self.server_script_path = server_script_path

        # å¯ç”¨ MCP tools åˆ—è¡¨
        self.available_tools = []

        # tool ä¸ server çš„æ˜ å°„
        self.tool_map = {}

    def get_server_parameters(self, path) -> StdioServerParameters:
        """
        æ ¹æ®æœåŠ¡å™¨è„šæœ¬æ–‡ä»¶ç±»å‹ç”Ÿæˆå¯åŠ¨æœåŠ¡å™¨æ‰€éœ€çš„å‚æ•°ã€‚

        :param path: æœåŠ¡å™¨è„šæœ¬æ–‡ä»¶è·¯å¾„
        :return: åŒ…å«æœåŠ¡å™¨å¯åŠ¨å‘½ä»¤å’Œå‚æ•°çš„ StdioServerParameters å¯¹è±¡
        :raises ValueError: å¦‚æœè„šæœ¬æ–‡ä»¶ç±»å‹ä¸æ˜¯ .py æˆ– .jar
        """
        if path.endswith('.py'):
            command = "python"
            args = [path]
        elif path.endswith('.jar'):
            command = "java"
            args = [
                "-Dfile.encoding=UTF-8",
                "-jar", path,
                "-Dspring.ai.mcp.server.stdio=true",
                "-Dspring.main.web-application-type=none"
            ]
        else:
            raise ValueError("ä¸æ”¯æŒçš„è„šæœ¬æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ .py æˆ– .jar æ–‡ä»¶")

        return StdioServerParameters(command=command, args=args, env=None)

    async def connect_to_mcp_server(self, server_name, path):
        """
        è¿æ¥åˆ° MCP æœåŠ¡å™¨ï¼Œå¯åŠ¨æœåŠ¡å™¨å¹¶åˆ—å‡ºå¯ç”¨å·¥å…·ï¼Œå°†å·¥å…·æ³¨å…¥åˆ° LLM ä¸­ã€‚

        :param server_name: æœåŠ¡å™¨åç§°
        :param path: æœåŠ¡å™¨è„šæœ¬æ–‡ä»¶è·¯å¾„
        """
        # è·å–æœåŠ¡å™¨å¯åŠ¨å‚æ•°
        server_params = self.get_server_parameters(path)

        # å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶è·å–è¾“å…¥è¾“å‡ºæµ
        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        stdio, write = stdio_transport

        # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯ä¼šè¯
        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
        await session.initialize()

        # æŠŠå½“å‰ server çš„ session åŠ å…¥åˆ°å­—å…¸ï¼Œä¾¿äºåç»­æ ¹æ®toolå¯»æ‰¾server
        self.mcp_session[server_name] = {"session": session, "stdio": stdio, "write": write}

        # åˆ—å‡º MCP æœåŠ¡å™¨ä¸Šçš„å·¥å…·
        mcp_response = await session.list_tools()
        tools = mcp_response.tools
        print(f"å·²è¿æ¥ MCP æœåŠ¡å™¨{server_name}ï¼Œæ”¯æŒä»¥ä¸‹å·¥å…·ï¼š\n", [tool.name for tool in tools])

        # æ ¼å¼åŒ–å¯ç”¨å·¥å…·
        format_tools = format_available_tools(mcp_response)
        for format_tool in format_tools:
            self.available_tools.append(format_tool)

        # æ„å»º tool ä¸ server çš„æ˜ å°„
        for tool in tools:
            self.tool_map[tool.name] = server_name


    def set_llm_tools(self):
        """
        å°†å¯ç”¨å·¥å…·æ³¨å…¥åˆ° LLM ä¸­ã€‚
        """
        if self.available_tools:
            self.llm_app.set_tools(self.available_tools)

    async def process_query(self, query: str) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ï¼Œè‹¥ LLM éœ€è¦è°ƒç”¨ MCP æœåŠ¡å™¨å·¥å…·ï¼Œåˆ™å¹¶å‘æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æ•´åˆç»“æœã€‚

        :param query: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢è¯­å¥
        :return: å¤„ç†åçš„å“åº”æ–‡æœ¬
        """
        # å‘ LLM å‘é€æŸ¥è¯¢å¹¶è·å–å“åº”
        response = self.llm_app.generate_response(prompt_index=1, user_input=query)
        content = response.choices[0]

        # å¦‚æœ LLM éœ€è¦è°ƒç”¨ MCP æœåŠ¡å™¨å·¥å…·
        if content.finish_reason == "tool_calls":
            # è§£ææ‰€æœ‰å·¥å…·è°ƒç”¨
            tool_calls = handle_tool_call(content)

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            tool_results = await self.run_tools_concurrently(tool_calls)
            # print(tool_results)

            # å°†å·¥å…·è°ƒç”¨ç»“æœå’Œåˆå§‹æŸ¥è¯¢é‡æ–°å‘é€ç»™ LLM
            response = self.llm_app.generate_response(
                prompt_index=2,
                user_input=f"{query}\n{tool_results}"
            )

        # è·å–æœ€ç»ˆå“åº”æ–‡æœ¬
        response_text = response.choices[0].message.content
        return response_text

    async def run_tools_concurrently(self, tool_calls: List[Dict]) -> List[str]:
        """
        å¹¶å‘æ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œå¹¶å¤„ç†å¯èƒ½å‡ºç°çš„å¼‚å¸¸ã€‚

        :param tool_calls: åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        :return: å·¥å…·è°ƒç”¨ç»“æœåˆ—è¡¨
        """
        tasks = []
        for tool_call in tool_calls:
            try:
                tool_name = tool_call.get("name")
                # print(tool_name)

                # ç¡®å®šå½“å‰å·¥å…·ä½¿ç”¨å“ªä¸€ä¸ª server çš„ session
                session = self.mcp_session.get(self.tool_map.get(tool_name)).get("session")

                # åˆ›å»ºå·¥å…·è°ƒç”¨ä»»åŠ¡
                task = session.call_tool(
                    tool_name,
                    tool_call.get("args")
                )
                tasks.append(task)
            except Exception as e:
                print(f"å·¥å…· {tool_call['name']} å‡†å¤‡ä»»åŠ¡å¤±è´¥: {str(e)}")

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)

        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # å¤„ç†å·¥å…·è°ƒç”¨å¤±è´¥çš„æƒ…å†µ
                processed_results.append(f"å·¥å…· {tool_calls[i]['name']} è°ƒç”¨å¤±è´¥: {str(result)}")
            else:
                # æå–å·¥å…·è°ƒç”¨æˆåŠŸçš„ç»“æœ
                processed_results.append(result.content[0].text)

        return processed_results

    async def chat_loop(self):
        """
        è¿è¡Œäº¤äº’å¼èŠå¤©å¾ªç¯ï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶å¤„ç†æŸ¥è¯¢ï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥ 'quit' é€€å‡ºã€‚
        """
        print("\nMCP å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼è¾“å…¥ 'quit' é€€å‡º")
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break

                # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
                response = await self.process_query(query)
                print(f"\nğŸ¤– AI: {response}")
            except Exception as e:
                print(f"\nâš ï¸ å‘ç”Ÿé”™è¯¯: {str(e)}")

    async def cleanup(self):
        """
        æ¸…ç†èµ„æºï¼Œå…³é—­å¼‚æ­¥ä¸Šä¸‹æ–‡ã€‚
        """
        await self.exit_stack.aclose()

    async def connect_to_all_servers(self):
        """
        è¿æ¥åˆ°æ‰€æœ‰ MCP æœåŠ¡å™¨ï¼Œå¹¶è®¾ç½® LLM å·¥å…·ã€‚
        """
        for path in self.server_script_path:
            print(path)
            await self.connect_to_mcp_server(path, path)

        self.set_llm_tools()


async def main():
    # åˆå§‹åŒ– LLM
    llm = AIApp(config_file='llm_model/config.ini')

    # æœåŠ¡å™¨è„šæœ¬è·¯å¾„åˆ—è¡¨
    server_script_path = [
        "mcp_server/mcp-server-0.0.1-SNAPSHOT.jar",
        "mcp_server/mcp_server_starter.py"
    ]

    # åˆ›å»º MCP å®¢æˆ·ç«¯å®ä¾‹
    client = MCPClient(llm=llm, server_script_path=server_script_path)

    try:
        # è¿æ¥åˆ°æ‰€æœ‰ MCP æœåŠ¡å™¨
        await client.connect_to_all_servers()

        # å¯åŠ¨èŠå¤©å¾ªç¯
        await client.chat_loop()
    finally:
        # æ¸…ç†èµ„æº
        await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())